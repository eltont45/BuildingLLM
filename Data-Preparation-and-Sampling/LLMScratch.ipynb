{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e10dd920-367a-446b-b4fe-b38edce9b3f1",
   "metadata": {},
   "source": [
    "# Reading in a short story as text sample in Python."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c0a49f-b116-4c1c-9cbf-4bbfcda98b71",
   "metadata": {},
   "source": [
    "## Step 1: Creating Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21897dee-2ffc-4d19-ae15-8e71d085715c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of character: 20479\n",
      "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no \n"
     ]
    }
   ],
   "source": [
    "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "print(\"Total number of character:\", len(raw_text))\n",
    "print(raw_text[:99])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89e5e4cc-b93e-4cc4-bc6f-2cf7a8152122",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #d4edda; color: #155724; padding: 15px; border: 2px solid #28a745; border-radius: 5px;\">\n",
    "Our goal is to tokenize this 20,479 character short story into individual words and special characters that we can then turn into embeddings for LLM training.\n",
    "We note that it is common to process millions of articles and hundreds of thousands of books -- many gigabytes of text -- when workings with LLMs. However, for this project we will we working with smaller text samples to illustrate the main points.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d2ade04a-bd65-4c92-9c75-d494a6f88c93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello,', ' ', 'world.', ' ', 'This,', ' ', 'is', ' ', 'a', ' ', 'test.']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "text = \"Hello, world. This, is a test.\"\n",
    "result = re.split(r'(\\s)', text) # splitting at white spaces\n",
    "\n",
    "print(result) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cea75c20-5865-4f68-a5ed-1044e3234fdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', '', ' ', 'world', '.', '', ' ', 'This', ',', '', ' ', 'is', ' ', 'a', ' ', 'test', '.', '']\n"
     ]
    }
   ],
   "source": [
    "result = re.split(r'([,.]|\\s)', text) #splitting at , and . now as well as white spaces\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9012cbbf-d1ac-419f-ab7a-a77782130249",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', 'world', '.', 'This', ',', 'is', 'a', 'test', '.']\n"
     ]
    }
   ],
   "source": [
    "result = [item for item in result if item.strip()] # remove redundant white spaces from list\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fabe095-9748-46bf-a078-79a2ed3d7230",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #d4edda; color: #155724; padding: 15px; border: 2px solid #28a745; border-radius: 5px;\">\n",
    "Removing whitespaces for tokenizer or keeping them as separate characters depends on our application and its requirements. Removing whitespaces reduces the memory and computing requirements. However, keeping whitespaces can be useful if we train models that are sensitive to the exact structure of the text (for example, Python code, which is sensitive to indentation and spacing. For now, we will remove whitespaces for simplicity.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b33ac144-d74f-4352-83c4-d5ffc6f845dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', '', ' ', 'world', '.', '', ' ', 'Is', ' ', 'this', '--', '', ' ', 'a', ' ', 'test', '?', '']\n"
     ]
    }
   ],
   "source": [
    "text = \"Hello, world. Is this-- a test?\"\n",
    "result = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text) # we want to tokenize ? , . : ; ...etc separately\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "399fa71d-8a8e-448b-bac3-682fda7f9c5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', 'world', '.', 'Is', 'this', '--', 'a', 'test', '?']\n"
     ]
    }
   ],
   "source": [
    "result = [item for item in result if item.strip()] # remove whitespace from list\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c68487d0-8087-407b-b6af-38b9c4d3ba0a",
   "metadata": {},
   "source": [
    "**Now we have basic tokenizer, we apply to Edith Wharton's entire short story**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6940515e-04b1-4605-ba72-21517080ebdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius', '--', 'though', 'a', 'good', 'fellow', 'enough', '--', 'so', 'it', 'was', 'no', 'great', 'surprise', 'to', 'me', 'to', 'hear', 'that', ',', 'in']\n"
     ]
    }
   ],
   "source": [
    "preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', raw_text)\n",
    "preprocessed = [item.strip() for item in preprocessed if item.strip()] # remove whitespace\n",
    "print(preprocessed[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b1444f47-8013-4e68-9383-8889efe0babe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4690\n"
     ]
    }
   ],
   "source": [
    "print(len(preprocessed))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f1c5fb-72f3-413b-8da5-8a26eab81886",
   "metadata": {},
   "source": [
    "## Step 2: Creating Token IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "44d267a6-ed82-4e4f-a387-baab4f28da2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1130\n"
     ]
    }
   ],
   "source": [
    "all_words = sorted(set(preprocessed))\n",
    "vocab_size = len(all_words)\n",
    "\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0388cb87-4075-46ef-901b-140ea9c14826",
   "metadata": {},
   "source": [
    "**After determining vocabulary size is 1,130,  we create the vocabulary and print the first 51 entries for illustration purposes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ff15db6f-17c4-49a6-b281-e8c7d625814f",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {token:integer for integer, token in enumerate(all_words)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a0b03283-dac5-4418-a2bf-8c2e35baff6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('!', 0)\n",
      "('\"', 1)\n",
      "(\"'\", 2)\n",
      "('(', 3)\n",
      "(')', 4)\n",
      "(',', 5)\n",
      "('--', 6)\n",
      "('.', 7)\n",
      "(':', 8)\n",
      "(';', 9)\n",
      "('?', 10)\n",
      "('A', 11)\n",
      "('Ah', 12)\n",
      "('Among', 13)\n",
      "('And', 14)\n",
      "('Are', 15)\n",
      "('Arrt', 16)\n",
      "('As', 17)\n",
      "('At', 18)\n",
      "('Be', 19)\n",
      "('Begin', 20)\n",
      "('Burlington', 21)\n",
      "('But', 22)\n",
      "('By', 23)\n",
      "('Carlo', 24)\n",
      "('Chicago', 25)\n",
      "('Claude', 26)\n",
      "('Come', 27)\n",
      "('Croft', 28)\n",
      "('Destroyed', 29)\n",
      "('Devonshire', 30)\n",
      "('Don', 31)\n",
      "('Dubarry', 32)\n",
      "('Emperors', 33)\n",
      "('Florence', 34)\n",
      "('For', 35)\n",
      "('Gallery', 36)\n",
      "('Gideon', 37)\n",
      "('Gisburn', 38)\n",
      "('Gisburns', 39)\n",
      "('Grafton', 40)\n",
      "('Greek', 41)\n",
      "('Grindle', 42)\n",
      "('Grindles', 43)\n",
      "('HAD', 44)\n",
      "('Had', 45)\n",
      "('Hang', 46)\n",
      "('Has', 47)\n",
      "('He', 48)\n",
      "('Her', 49)\n",
      "('Hermia', 50)\n"
     ]
    }
   ],
   "source": [
    "for i, item in enumerate(vocab.items()):\n",
    "    print(item)\n",
    "    if i >= 50:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494f1302-f302-458b-9231-a21d110616b8",
   "metadata": {},
   "source": [
    "**The dictionary contains individual tokens associated with unique integer labels**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c29447-107a-4618-903c-5a745d01c911",
   "metadata": {},
   "source": [
    "**Later when we want to convert outputs of LLM from numbers back to text, we also need a way to turn token IDs into text.**\n",
    "\n",
    "**To do this, we can create an inverse version of the vocabulary that maps token IDs back to corresponding text tokens.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6670c5dd-e2eb-4089-9119-6d737946fae0",
   "metadata": {},
   "source": [
    "**Now we will implement a complete tokenizer class in Python.**\n",
    "\n",
    "**The class will have an encode method that splits text into tokens and carries out the string-to-integer mapping to produce token IDs.**\n",
    "\n",
    "**We will also implement a decode method that carries out the reverse integer-to-string mapping to convert token IDs back to their tokens.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9ad50086-2f08-4680-9429-9542ac484fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizerV1:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = {i:s for s,i in vocab.items()}\n",
    "\n",
    "    def encode(self, text):\n",
    "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "\n",
    "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "        ids = [self.str_to_int[s] for s in preprocessed]\n",
    "        return ids\n",
    "\n",
    "    def decode(self, ids):\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "        # replace spaces before the specified punctuations\n",
    "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ae1dc7-fa1c-481e-b7c8-04d00719abd3",
   "metadata": {},
   "source": [
    "**Let's instantiate a new tokenizer object from the SimpleTokenizerV1 class and tokenize a passage from the short story**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "064c3108-c6fe-4fcd-972b-fc661a08f62f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 56, 2, 850, 988, 602, 533, 746, 5, 1126, 596, 5, 1, 67, 7, 38, 851, 1108, 754, 793, 7]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = SimpleTokenizerV1(vocab)\n",
    "\n",
    "text = \"\"\"\"It's the last he painted, you know,\"\n",
    "           Mrs. Gisburn said with pardonable pride.\"\"\"\n",
    "ids = tokenizer.encode(text)\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "baea3eec-34de-45ca-9f1b-d58e468c1827",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\" It\\' s the last he painted, you know,\" Mrs. Gisburn said with pardonable pride.'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c2b5af-0f4c-4745-bde5-39885d67ed80",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #d4edda; color: #155724; padding: 15px; border: 2px solid #28a745; border-radius: 5px;\">\n",
    "Important to note that there are many words not used in vocabulary. We need large and diverse training sets to extend the vocabulary when working on LLMs.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8aedae2-71f8-4330-88a9-9e0a96c383aa",
   "metadata": {},
   "source": [
    "## ADDING SPECIAL CONTEXT TOKENS\n",
    "\n",
    "One way to deal with unknown words is by using special context tokens\n",
    "\n",
    "We will modify the vocabulary and tokenizer in SimpleTokenizerV1 to SimpleTokenizerV2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed58fb5-69bd-4af8-8f5b-7e459f5ad1bd",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #d4edda; color: #155724; padding: 15px; border: 2px solid #28a745; border-radius: 5px;\">\n",
    "We can modify tokenizer to use an <|unk|> token if it encounters a word that is not part of the vocabulary.\n",
    "\n",
    "Furthermore, we add an <|endoftext|> between unrelated texts.\n",
    "\n",
    "For example, when training GPT-like LLMs on multiple independent documents or books, it is common to insert a token before each document or book that follows the previous text source\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "984276e1-0e6b-4ae7-9209-ee0447a7fda0",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tokens = sorted(list(set(preprocessed)))\n",
    "all_tokens.extend([\"<|endoftext|>\", \"<|unk|>\"])\n",
    "\n",
    "vocab = {token:integer for integer, token in enumerate(all_tokens)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d10e9973-0b23-4c2f-9cfb-7ee15735f761",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1132"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab.items())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b56c6a55-00e9-4ea7-adee-991ade2a4cf1",
   "metadata": {},
   "source": [
    "**As an additional check, let's print last 5 entries of updated vocabulary**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6c1fcd9c-c770-4816-b316-1b44b8d978fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('younger', 1127)\n",
      "('your', 1128)\n",
      "('yourself', 1129)\n",
      "('<|endoftext|>', 1130)\n",
      "('<|unk|>', 1131)\n"
     ]
    }
   ],
   "source": [
    "for i, item in enumerate(list(vocab.items())[-5:]):\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "343d9878-7511-428a-95a5-b9e8aaaa1ff1",
   "metadata": {},
   "source": [
    "**Now a simple text tokenizer that handles unknown words**\n",
    "\n",
    "**Replace unknown words by <|unk|> tokens**\n",
    "\n",
    "**Replaces spaces before the specified punctuations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7421ce41-c702-4ebe-800b-9e5446875a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizerV2:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = {i:s for s,i in vocab.items()}\n",
    "\n",
    "    def encode(self, text):\n",
    "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "        preprocessed = [\n",
    "            item if item in self.str_to_int\n",
    "            else \"<|unk|>\" for item in preprocessed\n",
    "        ]\n",
    "\n",
    "        ids = [self.str_to_int[s] for s in preprocessed]\n",
    "        return ids\n",
    "\n",
    "    def decode(self, ids):\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "        # replace spaces before the specified punctuations\n",
    "        text = re.sub(r'\\s+([,.:;?!\"()\\'])', r'\\1', text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "fa552fcb-3ef2-46ee-809e-04c882a6e082",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, do you like tea? <|endoftext|> In the sunlit terraces of the palace.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = SimpleTokenizerV2(vocab)\n",
    "\n",
    "text1 = \"Hello, do you like tea?\"\n",
    "text2 = \"In the sunlit terraces of the palace.\"\n",
    "\n",
    "text = \" <|endoftext|> \".join((text1, text2))\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2c8a67c9-d0c3-411a-a564-3982c3fdbdc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1131, 5, 355, 1126, 628, 975, 10, 1130, 55, 988, 956, 984, 722, 988, 1131, 7]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "626a6f35-9394-472d-a20b-f7228fbdb295",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|unk|>, do you like tea? <|endoftext|> In the sunlit terraces of the <|unk|>.'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenizer.encode(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc153d18-aff0-4a1a-8feb-26afee4ef3c5",
   "metadata": {},
   "source": [
    "Based on comparing the de-tokenized text above with original input text, we know the training dataset, Edith Wharton's The Verdict, did not contain the words \"Hello\" and \"palace\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59fdb5eb-93b5-4fd9-a0ae-66507ecbbe0a",
   "metadata": {},
   "source": [
    "**Other special tokens considered:**\n",
    "\n",
    "[BOS] (beginning of sequence): This token marks start of a text. Signifies to LLM where a piece of content begins.\n",
    "\n",
    "[EOS] (end of sequence): This token is positioned at the end of a text, and is especially useful for concatenating multiple unrelated texts.\n",
    "\n",
    "[PAD] (padding): When training LLMs with batch sizes larger than one, the batch might contain texts of varying lengths. To ensure all texts have the same length, the shorter texts are extended or \"padded\" using the [PAD] token, up to the length of the longest text in the batch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "124df5df-adb2-49c5-a9ba-6f265c2c249a",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #d4edda; color: #155724; padding: 15px; border: 2px solid #28a745; border-radius: 5px;\">\n",
    "Note that tokenizer used for GPT models do not need any of these tokens mentioned above but only use <|endoftext|> for simplicity\n",
    "\n",
    "Tokenizer used for GPT models also do not use <|unk|> token for out-of-vocabulary words. Instead, GPT models use a byte pair encoding tokenizer, which breaks down words into subword units\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eab1a6b-cbe6-446f-8b70-40f73146da0c",
   "metadata": {},
   "source": [
    "# Byte Pair Encoding (BPE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a07766-c6af-4a3b-a985-76188753d369",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #d4edda; color: #155724; padding: 15px; border: 2px solid #28a745; border-radius: 5px;\">\n",
    "This section covers a more sophisticated tokenization scheme based on concept called byte pair encoding (BPE).\n",
    "\n",
    "**BPE is a subword tokenization algorithm**\n",
    "\n",
    "BPE tokenizer covered here was used to train LLMs like GPT-2, GPT-3, and the original model used in ChatGPT.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe9ecd87-5077-448a-9db1-a1c4425a93d9",
   "metadata": {},
   "source": [
    "**BPE can be relatively complicated so we will use existing Python open-source library called tiktoken (https://github.com/openai/tiktoken)**\n",
    "\n",
    "The library implements BPE algorithm very efficiently based on source code in Rust."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60886849-7cfe-4928-9101-e6df15b85c44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tiktoken in /opt/anaconda3/lib/python3.12/site-packages (0.11.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /opt/anaconda3/lib/python3.12/site-packages (from tiktoken) (2024.11.6)\n",
      "Requirement already satisfied: requests>=2.26.0 in /opt/anaconda3/lib/python3.12/site-packages (from tiktoken) (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken) (2025.1.31)\n"
     ]
    }
   ],
   "source": [
    "! pip3 install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c6b2a294-4304-4eaa-9d65-1ae82eb6d4d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tiktoken version: 0.9.0\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import tiktoken\n",
    "\n",
    "print(\"tiktoken version:\", importlib.metadata.version(\"tiktoken\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "888100c8-1f6a-41bb-89c3-7bbc523db801",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #d4edda; color: #155724; padding: 15px; border: 2px solid #28a745; border-radius: 5px;\">\n",
    "Once installed, we can instantiate the BPE tokenizer from tiktoken as follows\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7be6646e-d599-4d29-840d-04cef68b90d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eba71dc-1b79-414c-bc7e-1c5f8eeffa4d",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #d4edda; color: #155724; padding: 15px; border: 2px solid #28a745; border-radius: 5px;\">\n",
    "The usage of this tokenizer is similar to SimpleTokenizerV2 we implemented previously via an encode method:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2d285890-e823-4fec-818d-86a0769ddf0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15496, 11, 466, 345, 588, 8887, 30, 220, 50256, 554, 262, 4252, 18250, 8812, 2114, 1659, 617, 34680, 27271, 13]\n"
     ]
    }
   ],
   "source": [
    "text = (\n",
    "    \"Hello, do you like tea? <|endoftext|> In the sunlit terraces\"\n",
    "     \"of someunknownPlace.\"\n",
    ")\n",
    "\n",
    "integers = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
    "\n",
    "print(integers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5361182-b2fd-4376-b0b6-37e36a3510e6",
   "metadata": {},
   "source": [
    "<div style=\"background-color: white; color: blue; padding: 15px; border: 2px solid blue; border-radius: 5px;\">\n",
    "We can then convert the token IDs back into text using the decode method similar to our SimpleTokenizerV2\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f192e76e-275c-4044-9300-4d953aba36fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, do you like tea? <|endoftext|> In the sunlit terracesof someunknownPlace.\n"
     ]
    }
   ],
   "source": [
    "strings = tokenizer.decode(integers)\n",
    "\n",
    "print(strings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27fbcb79-408a-467c-a22a-4e69f857b9be",
   "metadata": {},
   "source": [
    "<div style=\"background-color: white; color: orange; padding: 15px; border: 2px solid orange; border-radius: 5px;\">\n",
    "The BPE tokenizer, which was used to train models such as GPT-2, GPT-3, and the original model used in ChatGPT, has a <|endoftext|> being assigned with the largest token ID.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf64230-6eab-4b48-ab7b-834784fb9104",
   "metadata": {},
   "source": [
    "<div style=\"background-color: white; color: orange; padding: 15px; border: 2px solid orange; border-radius: 5px;\">\n",
    "The BPE tokenizer above encodes and decodes unknown words, such as \"someuknownPlace\" correctly.\n",
    "\n",
    "The BPE tokenizer can handle any unknown word. How does it achieve this without using <|unk|> tokens?\n",
    "\n",
    "The algorithm underlying BPE breaks down words that aren't in its predefined vocabulary into smaller subwords.\n",
    "This enables it to handle out-of-vocabulary words.\n",
    "Thanks to the BPE algorithm, if the tokenizer encounters an unfamiliar word during tokenization, it can represent it as a sequence of subword tokens or characters\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e34c78b-fef9-4674-958b-8fbd9808798d",
   "metadata": {},
   "source": [
    "**Another example to illustrate how the BPE tokenizer deals with unknown words**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "817afbf5-4504-470b-ae85-b500e2b96eb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[33901, 86, 343, 86, 220, 959]\n",
      "Akwirw ier\n"
     ]
    }
   ],
   "source": [
    "integers = tokenizer.encode(\"Akwirw ier\")\n",
    "print(integers)\n",
    "\n",
    "strings = tokenizer.decode(integers)\n",
    "print(strings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "093e24dd-bed5-436a-ad93-3bd6928390c2",
   "metadata": {},
   "source": [
    "## Creating input-target pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f7b9bd-557b-45dc-bd12-517b8edec505",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #d4edda; color: #155724; padding: 15px; border: 2px solid #28a745; border-radius: 5px;\">\n",
    "In this section we implement a data loader that fetches the input-target pairs using a sliding window approach.\n",
    "\n",
    "To get started, we will first tokenize the whole The Verdict short story we worked with earlier using the BPE tokenizer introduced in the previous section.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "46b05508-355d-4a55-a3d6-2402d7338070",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5145\n"
     ]
    }
   ],
   "source": [
    "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "enc_text = tokenizer.encode(raw_text)\n",
    "print(len(enc_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7117020a-01ad-403c-8432-a35b14148f52",
   "metadata": {},
   "source": [
    "<div style=\"background-color: white; color: blue; padding: 15px; border: 2px solid blue; border-radius: 5px;\">\n",
    "Executing the code above will return 5145. 5145 is the vocabulary size for The Verdict text or total number of tokens in the training set, after applying the BPE tokenizer.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "311414c9-c705-4098-9ec0-b3401a197d66",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #d4edda; color: #155724; padding: 15px; border: 2px solid #28a745; border-radius: 5px;\">\n",
    "Next, we remove the first 50 tokens from the dataset for demonstration purposes as it results a slightly more interesting text passage\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7c0b9793-2e6e-482c-b89c-2bdef9727dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_sample = enc_text[50:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75544520-9c79-49a8-ac7b-de9dba8c0f43",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #d4edda; color: #155724; padding: 15px; border: 2px solid #28a745; border-radius: 5px;\">\n",
    "One of the easiest and most intuitive ways to create input-target pairs for next word prediction task is to create x and y variables where x contains the input tokens and y contains the targets, which are the inputs shifted by 1:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9503ac60-2a3c-4a09-bf4b-6802fd416392",
   "metadata": {},
   "source": [
    "<div style=\"background-color: white; color: blue; padding: 15px; border: 2px solid blue; border-radius: 5px;\">\n",
    "The context size determines how many tokens are included in the input\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2b70580f-37ec-4fd8-bcc5-868710db5592",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: [290, 4920, 2241, 287]\n",
      "y:      [4920, 2241, 287, 257]\n"
     ]
    }
   ],
   "source": [
    "context_size = 4 #length of input\n",
    "#The context_size of 4 means that the model is trained to look at a sequence of 4 words (or tokens)\n",
    "#to predict the next word in the sequence.\n",
    "#The input x is the first 4 tokens [1,2,3,4], and the target y is the next 4 tokens [2,3,4,5]\n",
    "\n",
    "x = enc_sample[:context_size]\n",
    "y = enc_sample[1:context_size+1]\n",
    "\n",
    "print(f\"x: {x}\")\n",
    "print(f\"y:      {y}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68eadc6a-addf-4ac0-bf30-1da1e33a04fd",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #d4edda; color: #155724; padding: 15px; border: 2px solid #28a745; border-radius: 5px;\">\n",
    "Processing the inputs along with the targets, which are the inputs shifted by one position, we can then create the next-word prediction tasks as follows:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5ebde55e-77f3-4ad0-9b24-86d6aa3222de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[290] ----> 4920\n",
      "[290, 4920] ----> 2241\n",
      "[290, 4920, 2241] ----> 287\n",
      "[290, 4920, 2241, 287] ----> 257\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, context_size+1):\n",
    "    context = enc_sample[:i]\n",
    "    desired = enc_sample[i]\n",
    "\n",
    "    print(context, \"---->\", desired)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a9143b-bc69-4de9-b62c-47cb853fc225",
   "metadata": {},
   "source": [
    "<div style=\"background-color: white; color: blue; padding: 15px; border: 2px solid blue; border-radius: 5px;\">\n",
    "Everything left of the arrow (---->) refers to the input an LLM would receive, and the token ID on the right side of the arrow represents the target token ID that the LLM is supposed to predict.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa6904f-bac6-4698-942d-ca096c4869d7",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #d4edda; color: #155724; padding: 15px; border: 2px solid #28a745; border-radius: 5px;\">\n",
    "For illustration purposes, we repeat the previous code but convert the token IDs into text:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4a932c50-6237-47b7-be63-00a9aa35b640",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " and ---->  established\n",
      " and established ---->  himself\n",
      " and established himself ---->  in\n",
      " and established himself in ---->  a\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, context_size+1):\n",
    "    context = enc_sample[:i]\n",
    "    desired = enc_sample[i]\n",
    "\n",
    "    print(tokenizer.decode(context), \"---->\", tokenizer.decode([desired]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc30f01-84d5-4e25-ae68-18f811b6ef43",
   "metadata": {},
   "source": [
    "<div style=\"background-color: white; color: orange; padding: 15px; border: 2px solid orange; border-radius: 5px;\">\n",
    "We have now created the input-target pairs that we can turn into use for LLM training.\n",
    "\n",
    "Next task before we can turn tokens into embeddings is to implement an efficient data loader that iterates over the input dataset and returns the inputs and targets as PyTorch tensors, which can be thought of as multidimensional arrays.\n",
    "\n",
    "We are interested in returning two tensors: an input tensor containing the text that the LLM sees and a target tensor that includes the targets for the LLM to predict.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "277bb9d8-b06a-44db-8642-f8252e1a7b81",
   "metadata": {},
   "source": [
    "## Implementing a Data Loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "513fedb8-94b3-43f4-995a-53478825027e",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #d4edda; color: #155724; padding: 15px; border: 2px solid #28a745; border-radius: 5px;\">\n",
    "For the efficient data loader implementation, we will use PyTorch's built-in Dataset and DataLoader classes\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f71be24a-ea38-40df-8c4c-51ffebedcabd",
   "metadata": {},
   "source": [
    "<div style=\"background-color: white; color: blue; padding: 15px; border: 2px solid blue; border-radius: 5px;\">\n",
    "Step 1: Tokenize entire text\n",
    "\n",
    "\\\n",
    "Step 2: Use a sliding window to chunk the book into overlapping sequences of max_length\n",
    "\n",
    "Step 3: Return the total number of rows in the dataset\n",
    "\n",
    "Step 4: Return a single row from the dataset\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f013613e-568d-4f29-aecc-f60f1afb24f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self, txt, tokenizer, max_length, stride):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        # Tokenize the entire text\n",
    "        token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\n",
    "\n",
    "        # Use a sliding window to chunk the book into overlapping sequences of max_length\n",
    "        for i in range(0, len(token_ids) - max_length, stride):\n",
    "            input_chunk = token_ids[i:i + max_length]\n",
    "            target_chunk = token_ids[i + 1: i + max_length + 1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c05ddd-3596-4aa8-bdbc-6a68412960bb",
   "metadata": {},
   "source": [
    "<div style=\"background-color: white; color: orange; padding: 15px; border: 2px solid orange; border-radius: 5px;\">\n",
    "The GPTDatasetV1 in listing 2.5 is based on the PyTorch Dataset class.\n",
    "\n",
    "\\\n",
    "It defines how individual rows are fetched from the dataset.\n",
    "\n",
    "Each row consists of a number of token IDs (based on a max_length) assigned to an input_chunk tensor.\n",
    "\n",
    "The target_chunk tensor contains the corresponding targets.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b27f56-83a1-4d8f-b069-bd404c242300",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #d4edda; color: #155724; padding: 15px; border: 2px solid #28a745; border-radius: 5px;\">\n",
    "The following code will use the GPTDatasetV1 to load the inputs in batches via a Pytorch DataLoader:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c1d654-79de-4f9d-9d39-2f877438466c",
   "metadata": {},
   "source": [
    "<div style=\"background-color: white; color: blue; padding: 15px; border: 2px solid blue; border-radius: 5px;\">\n",
    "Step 1: Initialize the tokenizer\n",
    "\n",
    "\\\n",
    "Step 2: Create dataset\n",
    "\n",
    "Step 3: drop_last=True drops the last batch if it is shorter than the specified batch_size to prevent loss spikes during training\n",
    "\n",
    "Step 4: The number of CPU processes to use for preprocessing\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fb21a358-e8ee-4abf-b7da-bdf00109f2a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader_v1(txt, batch_size=4, max_length=256,\n",
    "                         stride=128, shuffle=True, drop_last=True,\n",
    "                         num_workers=0):\n",
    "\n",
    "    # Initialize the tokenizer\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "    # Create dataset\n",
    "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
    "\n",
    "    # Create dataloader\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,\n",
    "        num_workers=num_workers\n",
    "    )\n",
    "\n",
    "    return dataloader "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5cfb7b7-54e5-46b9-8b21-f5c1eda6ef12",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #d4edda; color: #155724; padding: 15px; border: 2px solid #28a745; border-radius: 5px;\">\n",
    "We test the dataloader with a batch size of 1 for an LLM with the context size of 4.\n",
    "This will develop an intuition of how the GPTDatasetV1 class and the create_dataloader_v1 function work together:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2623546d-18ad-4f20-a2a8-08829079906f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "786fb0e1-a429-4d0a-91e1-0b090e0c7ef4",
   "metadata": {},
   "source": [
    "<div style=\"background-color: white; color: blue; padding: 15px; border: 2px solid blue; border-radius: 5px;\">\n",
    "Convert dataloader into a Python iterator to fetch the next entry via Python's built-in next() function\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a3eb7c90-3871-4aba-b58b-86c364afa662",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.6.0\n",
      "[tensor([[  40,  367, 2885, 1464]]), tensor([[ 367, 2885, 1464, 1807]])]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "dataloader = create_dataloader_v1(\n",
    "    raw_text, batch_size=1, max_length=4, stride=1, shuffle=False\n",
    ")\n",
    "\n",
    "data_iter = iter(dataloader)\n",
    "first_batch = next(data_iter)\n",
    "print(first_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65086703-fa20-4a03-aa17-beb3d3e97fa4",
   "metadata": {},
   "source": [
    "<div style=\"background-color: white; color: orange; padding: 15px; border: 2px solid orange; border-radius: 5px;\">\n",
    "The first_batch variable contains two tensors: the first tensor stores the input token IDs and the second tensor stores the target token IDs.\n",
    "\n",
    "\\\n",
    "Since max_length is set to 4, each of the two tensors contains 4 token IDs.\n",
    "\n",
    "Note that an input size of 4 is relatively small and only chosen for illustration purposes. It is common to train LLMs with input sizes of at least 256.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd032bcb-40a5-42a5-a01e-edc93f8019a8",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #d4edda; color: #155724; padding: 15px; border: 2px solid #28a745; border-radius: 5px;\">\n",
    "To illustrate the meaning of stride=1, let's fetch another batch from this dataset:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f2be399b-7217-4a50-a875-99e3dd835420",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[ 367, 2885, 1464, 1807]]), tensor([[2885, 1464, 1807, 3619]])]\n"
     ]
    }
   ],
   "source": [
    "second_batch = next(data_iter)\n",
    "print(second_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f9a34a3-f555-4dc3-9506-0dfbb151928a",
   "metadata": {},
   "source": [
    "<div style=\"background-color: white; color: orange; padding: 15px; border: 2px solid orange; border-radius: 5px;\">\n",
    "If we compare the first with the second batch, we can see that the second batch's token IDs are shifted by one position compared to the first batch.\n",
    "\n",
    "\\\n",
    "For example, the second ID in the first batch's input is 367, which is the first ID of the second batch's input.\n",
    "\n",
    "The stride setting dictates the number of positions the inputs shift across batches, emulating a sliding window approach\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "005d5547-11aa-4078-b161-31e37bdb8018",
   "metadata": {},
   "source": [
    "<div style=\"background-color: white; color: orange; padding: 15px; border: 2px solid orange; border-radius: 5px;\">\n",
    "Batch sizes of 1, such as we have sampled from the data loader so far, are useful for illustration purposes.\n",
    "\n",
    "\\\n",
    "Small batch sizes require less memory during training but lead to more noisy model updates.\n",
    "\n",
    "**Batch size is a trade-off and hyperparameter to experiment with when training LLMs.**\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ffe2a2a-f141-452c-81e7-b515b08a6a33",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #d4edda; color: #155724; padding: 15px; border: 2px solid #28a745; border-radius: 5px;\">\n",
    "Let's have a look at how we can use the data loader to sample with a batch size greater than 1:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "efead112-39fc-467c-b772-7fac657138d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs:\n",
      " tensor([[   40,   367,  2885,  1464],\n",
      "        [ 1807,  3619,   402,   271],\n",
      "        [10899,  2138,   257,  7026],\n",
      "        [15632,   438,  2016,   257],\n",
      "        [  922,  5891,  1576,   438],\n",
      "        [  568,   340,   373,   645],\n",
      "        [ 1049,  5975,   284,   502],\n",
      "        [  284,  3285,   326,    11]])\n",
      "\n",
      "Targets:\n",
      " tensor([[  367,  2885,  1464,  1807],\n",
      "        [ 3619,   402,   271, 10899],\n",
      "        [ 2138,   257,  7026, 15632],\n",
      "        [  438,  2016,   257,   922],\n",
      "        [ 5891,  1576,   438,   568],\n",
      "        [  340,   373,   645,  1049],\n",
      "        [ 5975,   284,   502,   284],\n",
      "        [ 3285,   326,    11,   287]])\n"
     ]
    }
   ],
   "source": [
    "dataloader = create_dataloader_v1(raw_text, batch_size=8, max_length=4, stride=4, shuffle=False)\n",
    "\n",
    "data_iter = iter(dataloader)\n",
    "inputs, targets = next(data_iter)\n",
    "print(\"Inputs:\\n\", inputs)\n",
    "print(\"\\nTargets:\\n\", targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7305c34f-b6bc-4979-a5a8-21f0c87dba0a",
   "metadata": {},
   "source": [
    "<div style=\"background-color: white; color: blue; padding: 15px; border: 2px solid blue; border-radius: 5px;\">\n",
    "Note that we increase the stride to 4. This is to utilize the data set fully (we don't skip a single word) but also avoid any overlap between batches, since more overlap could lead to overfitting.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c6b661f-bbb2-4c04-a31e-c6cf7bcfe060",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
