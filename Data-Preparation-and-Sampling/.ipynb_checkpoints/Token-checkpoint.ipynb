{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e10dd920-367a-446b-b4fe-b38edce9b3f1",
   "metadata": {},
   "source": [
    "Reading in a short story as text sample in Python."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c0a49f-b116-4c1c-9cbf-4bbfcda98b71",
   "metadata": {},
   "source": [
    "Step 1: Creating Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21897dee-2ffc-4d19-ae15-8e71d085715c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of character: 20479\n",
      "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no \n"
     ]
    }
   ],
   "source": [
    "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "print(\"Total number of character:\", len(raw_text))\n",
    "print(raw_text[:99])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89e5e4cc-b93e-4cc4-bc6f-2cf7a8152122",
   "metadata": {},
   "source": [
    "Our goal is to tokenize this 20,479 character short story into individual words and special characters that we can then turn into embeddings for LLM training.\n",
    "We note that it is common to process millions of articles and hundreds of thousands of books -- many gigabytes of text -- when workings with LLMs. However, for this project we will we working with smaller text samples to illustrate the main points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d2ade04a-bd65-4c92-9c75-d494a6f88c93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello,', ' ', 'world.', ' ', 'This,', ' ', 'is', ' ', 'a', ' ', 'test.']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "text = \"Hello, world. This, is a test.\"\n",
    "result = re.split(r'(\\s)', text) # splitting at white spaces\n",
    "\n",
    "print(result) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cea75c20-5865-4f68-a5ed-1044e3234fdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', '', ' ', 'world', '.', '', ' ', 'This', ',', '', ' ', 'is', ' ', 'a', ' ', 'test', '.', '']\n"
     ]
    }
   ],
   "source": [
    "result = re.split(r'([,.]|\\s)', text) #splitting at , and . now as well as white spaces\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9012cbbf-d1ac-419f-ab7a-a77782130249",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', 'world', '.', 'This', ',', 'is', 'a', 'test', '.']\n"
     ]
    }
   ],
   "source": [
    "result = [item for item in result if item.strip()] # remove redundant white spaces from list\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fabe095-9748-46bf-a078-79a2ed3d7230",
   "metadata": {},
   "source": [
    "Removing whitespaces for tokenizer or keeping them as separate characters depends on our application and its requirements. Removing whitespaces reduces the memory and computing requirements. However, keeping whitespaces can be useful if we train models that are sensitive to the exact structure of the text (for example, Python code, which is sensitive to indentation and spacing. For now, we will remove whitespaces for simplicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b33ac144-d74f-4352-83c4-d5ffc6f845dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', '', ' ', 'world', '.', '', ' ', 'Is', ' ', 'this', '--', '', ' ', 'a', ' ', 'test', '?', '']\n"
     ]
    }
   ],
   "source": [
    "text = \"Hello, world. Is this-- a test?\"\n",
    "result = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text) # we want to tokenize ? , . : ; ...etc separately\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "399fa71d-8a8e-448b-bac3-682fda7f9c5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', 'world', '.', 'Is', 'this', '--', 'a', 'test', '?']\n"
     ]
    }
   ],
   "source": [
    "result = [item for item in result if item.strip()] # remove whitespace from list\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c68487d0-8087-407b-b6af-38b9c4d3ba0a",
   "metadata": {},
   "source": [
    "Now we have basic tokenizer, we apply to Edith Wharton's entire short story"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6940515e-04b1-4605-ba72-21517080ebdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius', '--', 'though', 'a', 'good', 'fellow', 'enough', '--', 'so', 'it', 'was', 'no', 'great', 'surprise', 'to', 'me', 'to', 'hear', 'that', ',', 'in']\n"
     ]
    }
   ],
   "source": [
    "preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', raw_text)\n",
    "preprocessed = [item.strip() for item in preprocessed if item.strip()] # remove whitespace\n",
    "print(preprocessed[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b1444f47-8013-4e68-9383-8889efe0babe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4690\n"
     ]
    }
   ],
   "source": [
    "print(len(preprocessed))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f1c5fb-72f3-413b-8da5-8a26eab81886",
   "metadata": {},
   "source": [
    "Step 2: Creating Token IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "44d267a6-ed82-4e4f-a387-baab4f28da2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1130\n"
     ]
    }
   ],
   "source": [
    "all_words = sorted(set(preprocessed))\n",
    "vocab_size = len(all_words)\n",
    "\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0388cb87-4075-46ef-901b-140ea9c14826",
   "metadata": {},
   "source": [
    "After determining vocabulary size is 1,130,  we create the vocabulary and print the first 51 entries for illustration purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ff15db6f-17c4-49a6-b281-e8c7d625814f",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {token:integer for integer, token in enumerate(all_words)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a0b03283-dac5-4418-a2bf-8c2e35baff6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('!', 0)\n",
      "('\"', 1)\n",
      "(\"'\", 2)\n",
      "('(', 3)\n",
      "(')', 4)\n",
      "(',', 5)\n",
      "('--', 6)\n",
      "('.', 7)\n",
      "(':', 8)\n",
      "(';', 9)\n",
      "('?', 10)\n",
      "('A', 11)\n",
      "('Ah', 12)\n",
      "('Among', 13)\n",
      "('And', 14)\n",
      "('Are', 15)\n",
      "('Arrt', 16)\n",
      "('As', 17)\n",
      "('At', 18)\n",
      "('Be', 19)\n",
      "('Begin', 20)\n",
      "('Burlington', 21)\n",
      "('But', 22)\n",
      "('By', 23)\n",
      "('Carlo', 24)\n",
      "('Chicago', 25)\n",
      "('Claude', 26)\n",
      "('Come', 27)\n",
      "('Croft', 28)\n",
      "('Destroyed', 29)\n",
      "('Devonshire', 30)\n",
      "('Don', 31)\n",
      "('Dubarry', 32)\n",
      "('Emperors', 33)\n",
      "('Florence', 34)\n",
      "('For', 35)\n",
      "('Gallery', 36)\n",
      "('Gideon', 37)\n",
      "('Gisburn', 38)\n",
      "('Gisburns', 39)\n",
      "('Grafton', 40)\n",
      "('Greek', 41)\n",
      "('Grindle', 42)\n",
      "('Grindles', 43)\n",
      "('HAD', 44)\n",
      "('Had', 45)\n",
      "('Hang', 46)\n",
      "('Has', 47)\n",
      "('He', 48)\n",
      "('Her', 49)\n",
      "('Hermia', 50)\n"
     ]
    }
   ],
   "source": [
    "for i, item in enumerate(vocab.items()):\n",
    "    print(item)\n",
    "    if i >= 50:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494f1302-f302-458b-9231-a21d110616b8",
   "metadata": {},
   "source": [
    "The dictionary contains individual tokens associated with unique integer labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c29447-107a-4618-903c-5a745d01c911",
   "metadata": {},
   "source": [
    "Later when we want to convert outputs of LLM from numbers back to text, we also need a way to turn token IDs into text.\n",
    "\n",
    "To do this, we can create an inverse version of the vocabulary that maps token IDs back to corresponding text tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6670c5dd-e2eb-4089-9119-6d737946fae0",
   "metadata": {},
   "source": [
    "Now we will implement a complete tokenizer class in Python.\n",
    "\n",
    "The class will have an encode method that splits text into tokens and carries out the string-to-integer mapping to produce token IDs.\n",
    "\n",
    "We will also implement a decode method that carries out the reverse integer-to-string mapping to convert token IDs back to their tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9ad50086-2f08-4680-9429-9542ac484fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizerV1:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = {i:s for s,i in vocab.items()}\n",
    "\n",
    "    def encode(self, text):\n",
    "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "\n",
    "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "        ids = [self.str_to_int[s] for s in preprocessed]\n",
    "        return ids\n",
    "\n",
    "    def decode(self, ids):\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "        # replace spaces before the specified punctuations\n",
    "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ae1dc7-fa1c-481e-b7c8-04d00719abd3",
   "metadata": {},
   "source": [
    "Let's instantiate a new tokenizer object from the SimpleTokenizerV1 class and tokenize a passage from the short story"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "064c3108-c6fe-4fcd-972b-fc661a08f62f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 56, 2, 850, 988, 602, 533, 746, 5, 1126, 596, 5, 1, 67, 7, 38, 851, 1108, 754, 793, 7]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = SimpleTokenizerV1(vocab)\n",
    "\n",
    "text = \"\"\"\"It's the last he painted, you know,\"\n",
    "           Mrs. Gisburn said with pardonable pride.\"\"\"\n",
    "ids = tokenizer.encode(text)\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "baea3eec-34de-45ca-9f1b-d58e468c1827",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\" It\\' s the last he painted, you know,\" Mrs. Gisburn said with pardonable pride.'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c2b5af-0f4c-4745-bde5-39885d67ed80",
   "metadata": {},
   "source": [
    "Important to note that there are many words not used in vocabulary. We need large and diverse training sets to extend the vocabulary when working on LLMs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8aedae2-71f8-4330-88a9-9e0a96c383aa",
   "metadata": {},
   "source": [
    "ADDING SPECIAL CONTEXT TOKENS\n",
    "\n",
    "One way to deal with unknown words is by using special context tokens\n",
    "\n",
    "We will modify the vocabulary and tokenizer in SimpleTokenizerV1 to SimpleTokenizerV2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed58fb5-69bd-4af8-8f5b-7e459f5ad1bd",
   "metadata": {},
   "source": [
    "We can modify tokenizer to use an <|unk|> token if it encounters a word that is not part of the vocabulary.\n",
    "\n",
    "Furthermore, we add an <|endoftext|> between unrelated texts.\n",
    "\n",
    "For example, when training GPT-like LLMs on multiple independent documents or books, it is common to insert a token before each document or book that follows the previous text source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "984276e1-0e6b-4ae7-9209-ee0447a7fda0",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tokens = sorted(list(set(preprocessed)))\n",
    "all_tokens.extend([\"<|endoftext|>\", \"<|unk|>\"])\n",
    "\n",
    "vocab = {token:integer for integer, token in enumerate(all_tokens)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d10e9973-0b23-4c2f-9cfb-7ee15735f761",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1132"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab.items())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b56c6a55-00e9-4ea7-adee-991ade2a4cf1",
   "metadata": {},
   "source": [
    "As an additional check, let's print last 5 entries of updated vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6c1fcd9c-c770-4816-b316-1b44b8d978fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('younger', 1127)\n",
      "('your', 1128)\n",
      "('yourself', 1129)\n",
      "('<|endoftext|>', 1130)\n",
      "('<|unk|>', 1131)\n"
     ]
    }
   ],
   "source": [
    "for i, item in enumerate(list(vocab.items())[-5:]):\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "343d9878-7511-428a-95a5-b9e8aaaa1ff1",
   "metadata": {},
   "source": [
    "Now a simple text tokenizer that handles unknown words\n",
    "\n",
    "Replace unknown words by <|unk|> tokens\n",
    "\n",
    "Replaces spaces before the specified punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7421ce41-c702-4ebe-800b-9e5446875a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizerV2:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = {i:s for s,i in vocab.items()}\n",
    "\n",
    "    def encode(self, text):\n",
    "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "        preprocessed = [\n",
    "            item if item in self.str_to_int\n",
    "            else \"<|unk|>\" for item in preprocessed\n",
    "        ]\n",
    "\n",
    "        ids = [self.str_to_int[s] for s in preprocessed]\n",
    "        return ids\n",
    "\n",
    "    def decode(self, ids):\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "        # replace spaces before the specified punctuations\n",
    "        text = re.sub(r'\\s+([,.:;?!\"()\\'])', r'\\1', text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "fa552fcb-3ef2-46ee-809e-04c882a6e082",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, do you like tea? <|endoftext|> In the sunlit terraces of the palace.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = SimpleTokenizerV2(vocab)\n",
    "\n",
    "text1 = \"Hello, do you like tea?\"\n",
    "text2 = \"In the sunlit terraces of the palace.\"\n",
    "\n",
    "text = \" <|endoftext|> \".join((text1, text2))\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2c8a67c9-d0c3-411a-a564-3982c3fdbdc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1131, 5, 355, 1126, 628, 975, 10, 1130, 55, 988, 956, 984, 722, 988, 1131, 7]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "626a6f35-9394-472d-a20b-f7228fbdb295",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|unk|>, do you like tea? <|endoftext|> In the sunlit terraces of the <|unk|>.'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenizer.encode(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc153d18-aff0-4a1a-8feb-26afee4ef3c5",
   "metadata": {},
   "source": [
    "Based on comparing the de-tokenized text above with original input text, we know the training dataset, Edith Wharton's The Verdict, did not contain the words \"Hello\" and \"palace\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59fdb5eb-93b5-4fd9-a0ae-66507ecbbe0a",
   "metadata": {},
   "source": [
    "Other special tokens considered:\n",
    "\n",
    "[BOS] (beginning of sequence): This token marks start of a text. Signifies to LLM where a piece of content begins.\n",
    "\n",
    "[EOS] (end of sequence): This token is positioned at the end of a text, and is especially useful for concatenating multiple unrelated texts.\n",
    "\n",
    "[PAD] (padding): When training LLMs with batch sizes larger than one, the batch might contain texts of varying lengths. To ensure all texts have the same length, the shorter texts are extended or \"padded\" using the [PAD] token, up to the length of the longest text in the batch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "124df5df-adb2-49c5-a9ba-6f265c2c249a",
   "metadata": {},
   "source": [
    "Note that tokenizer used for GPT models do not need any of these tokens mentioned above but only use <|endoftext|> for simplicity\n",
    "\n",
    "Tokenizer used for GPT models also do not use <|unk|> token for out-of-vocabulary words. Instead, GPT models use a byte pair encoding tokenizer, which breaks down words into subword units"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eab1a6b-cbe6-446f-8b70-40f73146da0c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c86c7bb6-e9d8-468d-82fc-b1c05dc9a566",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
